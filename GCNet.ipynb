{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5ed11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T12:23:06.974003Z",
     "iopub.status.busy": "2023-05-05T12:23:06.973621Z",
     "iopub.status.idle": "2023-05-05T12:23:08.558978Z",
     "shell.execute_reply": "2023-05-05T12:23:08.558089Z",
     "shell.execute_reply.started": "2023-05-05T12:23:06.973970Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from mmcv.cnn import constant_init, kaiming_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e9d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T12:23:08.561259Z",
     "iopub.status.busy": "2023-05-05T12:23:08.560776Z",
     "iopub.status.idle": "2023-05-05T12:23:24.098649Z",
     "shell.execute_reply": "2023-05-05T12:23:24.097721Z",
     "shell.execute_reply.started": "2023-05-05T12:23:08.561232Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 256\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=32,\n",
    "                          padding=4,       \n",
    "                          padding_mode='reflect'),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, pin_memory=True, num_workers=3)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size*2,\n",
    "                                         shuffle=False, pin_memory=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63229d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBlock2d(nn.Module):\n",
    "    def __init__(self, inplanes, planes, pool='att', fusion=['channel_add'], ratio=8):\n",
    "        super(ContextBlock2d, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.pool = pool\n",
    "        self.fusions = fusions\n",
    "        \n",
    "        if 'att' in pool:\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        if 'channel_add' in fusions:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes // ratio, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes // ratio, 1, 1]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(self.planes // ratio, self.inplanes, kernel_size=1)\n",
    "            )\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        if self.pool == 'att':\n",
    "            kaiming_init(self.conv_mask, mode='fan_in')\n",
    "            self.conv_mask.inited = True\n",
    "            \n",
    "        if self.channel_add_conv is not None:\n",
    "            last_zero_init(self.channel_add_conv)\n",
    "        if self.channel_mul_conv is not None:\n",
    "            last_zero_init(self.channel_mul_conv)\n",
    "    \n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pool == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)#softmax操作\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(3)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = x * channel_mul_term\n",
    "        else:\n",
    "            out = x\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, gc=False):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, block1, num_block, num_classes=100, gc=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer1(block, block1, 256, num_block[2], 2, gc=gc)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def _make_layer1(self, block, block1, planes, blocks, stride=1, gc=False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks-1):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        layers.append(block1(self.inplanes, self.inplanes))\n",
    "        layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53d789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T12:23:24.165363Z",
     "iopub.status.busy": "2023-05-05T12:23:24.164892Z",
     "iopub.status.idle": "2023-05-05T12:23:27.446766Z",
     "shell.execute_reply": "2023-05-05T12:23:27.444799Z",
     "shell.execute_reply.started": "2023-05-05T12:23:24.165332Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet18().to(device)\n",
    "num_param = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (num_param/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8152e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T12:23:27.448646Z",
     "iopub.status.busy": "2023-05-05T12:23:27.448309Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 30\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0002, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "# sched = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.2)\n",
    "sched = optim.lr_scheduler.OneCycleLR(optimizer, 0.01, epochs=epochs,\n",
    "                                      steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "fit(model=model,\n",
    "    epochs=epochs,\n",
    "    train_loader=trainloader,\n",
    "    valid_loader=testloader,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=sched,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    warm_up=False, max_warm_up_lr=0.01,\n",
    "    grad_clip=None, updata_lr_every_epoch=False, PATH='./SKNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(loss_arr)\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(3.5, 2.5))\n",
    "plt.plot(top1_acc_arr)\n",
    "plt.plot(top5_acc_arr)\n",
    "plt.legend(['Top1 acc','Top5 acc'])\n",
    "plt.title('Arr')\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(lr_arr)\n",
    "plt.title('LR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
