{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fec67c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3295692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 256\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=32,\n",
    "                          padding=4,       \n",
    "                          padding_mode='reflect'),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, pin_memory=True, num_workers=3)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size*2,\n",
    "                                         shuffle=False, pin_memory=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64008b1",
   "metadata": {},
   "source": [
    "# Res-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce070bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes,\n",
    "                     kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes,\n",
    "                     kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "    def __init__(self, in_planes, out_planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, out_planes, stride)\n",
    "        self.bn1= nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(True) # replace the original value\n",
    "        self.conv2 = conv3x3(out_planes, out_planes)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        else:\n",
    "            identity = x\n",
    "            \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, out_planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(in_planes, out_planes)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = conv3x3(out_planes, out_planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv3 = conv1x1(out_planes, out_planes*self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes*self.expansion)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        else:\n",
    "            identity = x\n",
    "            \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def make_layer(self, block, out_planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != out_planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_planes, out_planes*block.expansion, stride),\n",
    "                nn.BatchNorm2d(out_planes*block.expansion)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, out_planes, stride, downsample))\n",
    "        self.in_planes = out_planes*block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, out_planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def ResNet18():\n",
    "    model = ResNet(BasicBlock, [2,2,2,2])\n",
    "    return model\n",
    "def ResNet34():\n",
    "    model = ResNet(BasicBlock, [3,4,6,3])\n",
    "    return model\n",
    "def ResNet50():\n",
    "    model = ResNet(Bottleneck, [3,4,6,3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb2e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "def adjust_learning_rate(optimizer, current_iter, warmup_iter, max_warm_up_lr=0.1):\n",
    "    if current_iter <= warmup_iter:\n",
    "        lr = max_warm_up_lr * current_iter / warmup_iter\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97b58614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "acc_arr = []\n",
    "lr_arr = []\n",
    "\n",
    "def fit(model, epochs, train_loader, valid_loader, optimizer, lr_scheduler,\n",
    "        loss_fn, updata_lr_every_epoch, warm_up=False, max_warm_up_lr=0.01, grad_clip=None, PATH='./SKNet.pth'):\n",
    "    log_interval = int((50000/batch_size)/2)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"{'='*20} Epoch: {epoch+1} {'='*20}\\n\")\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            if warm_up != False and epoch <= warm_up:\n",
    "                adjust_learning_rate(optimizer, (i+1)+epoch*len(train_loader), len(train_loader)*warm_up, max_warm_up_lr=max_warm_up_lr)\n",
    "                lr_arr.append(get_lr(optimizer))\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = loss_fn(outputs, targets.to(device))\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            if lr_scheduler is not None and updata_lr_every_epoch == False:\n",
    "                lr_scheduler.step()\n",
    "                lr_arr.append(get_lr(optimizer))\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if i % log_interval == log_interval-1:\n",
    "                avg_loss = avg_loss/log_interval\n",
    "                print(f\"batch: {i+1}, train_loss: {avg_loss:.4f}\")\n",
    "                avg_loss = 0\n",
    "                \n",
    "        if lr_scheduler is not None and updata_lr_every_epoch == True:\n",
    "            lr_scheduler.step()\n",
    "            lr_arr.append(get_lr(optimizer))\n",
    "        if lr_scheduler is None:\n",
    "            lr_arr.append(get_lr(optimizer))\n",
    "        \n",
    "        model.eval()\n",
    "        top1_correct = 0\n",
    "        top5_correct = 0\n",
    "        total = 0\n",
    "        avg_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (images, labels) in valid_loader:\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images.to(device))\n",
    "                _, predicted = torch.topk(outputs.data, 5, dim=1, largest=True, sorted=True)\n",
    "                total += labels.cpu().size(0)\n",
    "                top1_correct += (predicted[:, 0] == labels).sum().item()\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i] in predicted[i]:\n",
    "                        top5_correct += 1\n",
    "                avg_loss += loss_fn(outputs, labels)\n",
    "            avg_loss = avg_loss.cpu() / len(valid_loader)\n",
    "            loss_arr.append(avg_loss)\n",
    "            top1_acc = 100 * top1_correct / total\n",
    "            top5_acc = 100 * top5_correct / total\n",
    "            acc_arr.append(top1_acc)\n",
    "            \n",
    "        print(f'Top1 Accuracy: {top1_acc}% ({top1_correct} / {total}), Top5 Accuracy: {top5_acc}% ({top5_correct} / {total}), Loss: {avg_loss:.3f}, Last_lr: {lr_arr[-1]:.5f}')\n",
    "    torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd792e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 11.23M\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18().to(device)\n",
    "num_param = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (num_param/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43853161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch: 1 ====================\n",
      "\n",
      "batch: 97, train_loss: 4.6933\n",
      "batch: 194, train_loss: 4.4189\n",
      "Top1 Accuracy: 8.37% (837 / 10000), Top5 Accuracy: 26.08% (2608 / 10000), Loss: 4.093, Last_lr: 0.00007\n",
      "==================== Epoch: 2 ====================\n",
      "\n",
      "batch: 97, train_loss: 4.2059\n",
      "batch: 194, train_loss: 4.0513\n",
      "Top1 Accuracy: 14.92% (1492 / 10000), Top5 Accuracy: 37.66% (3766 / 10000), Loss: 3.660, Last_lr: 0.00015\n",
      "==================== Epoch: 3 ====================\n",
      "\n",
      "batch: 97, train_loss: 3.9194\n",
      "batch: 194, train_loss: 3.7984\n",
      "Top1 Accuracy: 18.75% (1875 / 10000), Top5 Accuracy: 45.12% (4512 / 10000), Loss: 3.392, Last_lr: 0.00028\n",
      "==================== Epoch: 4 ====================\n",
      "\n",
      "batch: 97, train_loss: 3.6660\n",
      "batch: 194, train_loss: 3.5887\n",
      "Top1 Accuracy: 22.07% (2207 / 10000), Top5 Accuracy: 50.25% (5025 / 10000), Loss: 3.205, Last_lr: 0.00044\n",
      "==================== Epoch: 5 ====================\n",
      "\n",
      "batch: 97, train_loss: 3.4848\n",
      "batch: 194, train_loss: 3.4252\n",
      "Top1 Accuracy: 24.75% (2475 / 10000), Top5 Accuracy: 54.44% (5444 / 10000), Loss: 3.032, Last_lr: 0.00060\n",
      "==================== Epoch: 6 ====================\n",
      "\n",
      "batch: 97, train_loss: 3.3143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# sched = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.2)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sched \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(optimizer, \u001b[38;5;241m0.001\u001b[39m, epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m      6\u001b[0m                                       steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trainloader))\n\u001b[1;32m----> 8\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdata_lr_every_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./ResNet.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 22\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, epochs, train_loader, valid_loader, optimizer, lr_scheduler, loss_fn, updata_lr_every_epoch, warm_up, max_warm_up_lr, grad_clip, PATH)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(model\u001b[38;5;241m.\u001b[39mparameters(), grad_clip)\n\u001b[1;32m---> 22\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m updata_lr_every_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:435\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    433\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[0;32m    434\u001b[0m device_exp_avgs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avgs]\n\u001b[1;32m--> 435\u001b[0m device_exp_avg_sqs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avg_sqs]\n\u001b[0;32m    436\u001b[0m params_ \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# update steps\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda\\envs\\DL\\lib\\site-packages\\torch\\optim\\adam.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    433\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[0;32m    434\u001b[0m device_exp_avgs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avgs]\n\u001b[1;32m--> 435\u001b[0m device_exp_avg_sqs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avg_sqs]\n\u001b[0;32m    436\u001b[0m params_ \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# update steps\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "# optimizer = optim.SGD(model.parameters(), lr=max_lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "# sched = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.2)\n",
    "sched = optim.lr_scheduler.OneCycleLR(optimizer, 0.003, epochs=epochs,\n",
    "                                      steps_per_epoch=len(trainloader))\n",
    "\n",
    "fit(model=model,\n",
    "    epochs=epochs,\n",
    "    train_loader=trainloader,\n",
    "    valid_loader=testloader,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=sched,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    warm_up=False, grad_clip=None, updata_lr_every_epoch=False, PATH='./ResNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29998dfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(loss_arr)\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(acc_arr)\n",
    "plt.title('Arr')\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(lr_arr)\n",
    "plt.title('LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67413772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './ResNet.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faa62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
